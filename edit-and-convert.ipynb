{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "% % capture\n",
    "!python --version\n",
    "!pip install --upgrade jaxlib flax\n",
    "!pip install --upgrade \"jax[cuda]\" -f https: // storage.googleapis.com / jax-releases / jax_releases.html\n",
    "!pip install bert-pytorch msgpack tbp-nightly"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:03:25.346269Z",
     "iopub.execute_input": "2022-02-11T09:03:25.347118Z",
     "iopub.status.idle": "2022-02-11T09:04:19.807484Z",
     "shell.execute_reply.started": "2022-02-11T09:03:25.347011Z",
     "shell.execute_reply": "2022-02-11T09:04:19.806313Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "\n",
    "import flax.linen as nn\n",
    "import flax.serialization\n",
    "import flax.training.train_state as train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import msgpack\n",
    "import numpy as np\n",
    "import optax\n",
    "import tqdm\n",
    "\n",
    "if 'TPU_NAME' in os.environ:\n",
    "    import requests\n",
    "\n",
    "    if 'TPU_DRIVER_MODE' not in globals():\n",
    "        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n",
    "        resp = requests.post(url)\n",
    "        TPU_DRIVER_MODE = 1\n",
    "\n",
    "    from jax.config import config\n",
    "\n",
    "    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n",
    "    print('Registered TPU:', config.FLAGS.jax_backend_target)\n",
    "else:\n",
    "    print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')\n",
    "\n",
    "jax.devices()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:04:19.810222Z",
     "iopub.execute_input": "2022-02-11T09:04:19.810634Z",
     "iopub.status.idle": "2022-02-11T09:05:06.613998Z",
     "shell.execute_reply.started": "2022-02-11T09:04:19.810582Z",
     "shell.execute_reply": "2022-02-11T09:05:06.612766Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm.tqdm(texts):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = line.replace(\"\\n\", \" \").replace(\"\\t\", \" \").split()[:4]\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "        print(seq)\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def build():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--corpus_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-o\", \"--output_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-s\", \"--vocab_size\", type=int, default=None)\n",
    "    parser.add_argument(\"-e\", \"--encoding\", type=str, default=\"utf-8\")\n",
    "    parser.add_argument(\"-m\", \"--min_freq\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.corpus_path, \"r\", encoding=args.encoding) as f:\n",
    "        vocab = WordVocab(f, max_size=args.vocab_size, min_freq=args.min_freq)\n",
    "\n",
    "    print(\"VOCAB SIZE:\", len(vocab))\n",
    "    print(vocab)\n",
    "    vocab.save_vocab(args.output_path)"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:06.616214Z",
     "iopub.execute_input": "2022-02-11T09:05:06.616571Z",
     "iopub.status.idle": "2022-02-11T09:05:06.673753Z",
     "shell.execute_reply.started": "2022-02-11T09:05:06.616527Z",
     "shell.execute_reply": "2022-02-11T09:05:06.672239Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:06.676197Z",
     "iopub.execute_input": "2022-02-11T09:05:06.676701Z",
     "iopub.status.idle": "2022-02-11T09:05:08.355748Z",
     "shell.execute_reply.started": "2022-02-11T09:05:06.676656Z",
     "shell.execute_reply": "2022-02-11T09:05:08.354404Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration constants\n",
    "TESTING = False\n",
    "\n",
    "# Architecture-specific constants\n",
    "EMBEDDING_SIZE = 128\n",
    "ENCODER_HIDDEN_SIZE = 512\n",
    "INPUT_ENCODER_HIDDEN_SIZE = 1024\n",
    "\n",
    "# Model-specific constants\n",
    "MAX_ASSEMBLY_LINE_LENGTH = 10\n",
    "SEQ_LENGTH = EMBEDDING_SIZE * 2 + 4 * MAX_ASSEMBLY_LINE_LENGTH\n",
    "\n",
    "# Training related constants\n",
    "BATCH_SIZE = 8 if TESTING else 32\n",
    "TRAIN_STEPS = 30\n",
    "NUM_OF_TESTS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Palmtree related variables\n",
    "vocab = WordVocab.load_vocab(\"../input/palmtreevocab/vocab\")\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "EOS_ID = [0 for _ in range(MAX_ASSEMBLY_LINE_LENGTH)]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.357747Z",
     "iopub.execute_input": "2022-02-11T09:05:08.358082Z",
     "iopub.status.idle": "2022-02-11T09:05:08.366768Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.358044Z",
     "shell.execute_reply": "2022-02-11T09:05:08.365525Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.transforms.scan,\n",
    "        variable_broadcast='params',\n",
    "        in_axes=1,\n",
    "        out_axes=1,\n",
    "        split_rngs={'params': False})\n",
    "    @nn.compact\n",
    "    # x = (BATCH_SIZE, SEQ_LENGTH)\n",
    "    def __call__(self, carry, x):\n",
    "        lstm_state, is_eos = carry\n",
    "        new_lstm_state, y = nn.LSTMCell()(lstm_state, x)\n",
    "\n",
    "        def select_carried_state(new_state, old_state):\n",
    "            return jnp.where(is_eos[:, np.newaxis], old_state, new_state)\n",
    "\n",
    "        carried_lstm_state = tuple(select_carried_state(*s) for s in zip(new_lstm_state, lstm_state))\n",
    "        is_eos = jnp.logical_or(is_eos, jnp.any(x, axis=1))\n",
    "        return (carried_lstm_state, is_eos), y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        # use dummy key since default state init fn is just zeros.\n",
    "        return nn.LSTMCell.initialize_carry(\n",
    "            jax.random.PRNGKey(0), (batch_size,), hidden_size)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.368841Z",
     "iopub.execute_input": "2022-02-11T09:05:08.369690Z",
     "iopub.status.idle": "2022-02-11T09:05:08.394095Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.369645Z",
     "shell.execute_reply": "2022-02-11T09:05:08.392470Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    hidden_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        # inputs = (BATCH_SIZE,MODEL_LINE_COUNT,SEQ_LENGTH)\n",
    "        batch_size = inputs.shape[0]\n",
    "        init_lstm_state = EncoderLSTM.initialize_carry(batch_size, self.hidden_size)\n",
    "        init_is_eos = jnp.zeros(batch_size, dtype=np.bool)\n",
    "        init_carry = (init_lstm_state, init_is_eos)\n",
    "        (final_state, _), _ = EncoderLSTM()(init_carry, inputs)\n",
    "        return final_state"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.395944Z",
     "iopub.execute_input": "2022-02-11T09:05:08.396369Z",
     "iopub.status.idle": "2022-02-11T09:05:08.410791Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.396330Z",
     "shell.execute_reply": "2022-02-11T09:05:08.409533Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pad_output(l, max_length, index):\n",
    "    pad = np.zeros((max_length - l.shape[0], l.shape[1]))\n",
    "    pad[:, index] = 1\n",
    "    return jnp.concatenate((l, pad), axis=0)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.412850Z",
     "iopub.execute_input": "2022-02-11T09:05:08.413736Z",
     "iopub.status.idle": "2022-02-11T09:05:08.427479Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.413678Z",
     "shell.execute_reply": "2022-02-11T09:05:08.426139Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_data(path='../input/smallmsgpack-filtered/data.bin'):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = msgpack.Unpacker(f, raw=False, strict_map_key=False)\n",
    "\n",
    "        metadata = next(data)\n",
    "\n",
    "        MODEL_LINE_COUNT = max(metadata[\"max_line_count_O0\"], metadata[\"max_line_count_O2\"])\n",
    "        O0_MODEL_LINE_COUNT = metadata[\"max_line_count_O0\"]\n",
    "        OUTPUT_TOKEN_COUNT = MODEL_LINE_COUNT * MAX_ASSEMBLY_LINE_LENGTH\n",
    "\n",
    "        yield MODEL_LINE_COUNT, O0_MODEL_LINE_COUNT, OUTPUT_TOKEN_COUNT\n",
    "\n",
    "        O0 = np.zeros((BATCH_SIZE, O0_MODEL_LINE_COUNT + 1, EMBEDDING_SIZE))\n",
    "        inp = np.zeros((BATCH_SIZE, MODEL_LINE_COUNT + 1, SEQ_LENGTH))\n",
    "        outp = np.zeros((BATCH_SIZE, OUTPUT_TOKEN_COUNT, VOCAB_SIZE))\n",
    "\n",
    "        i = 0\n",
    "        for program in data:\n",
    "            for line in program.values():\n",
    "                # print(line)\n",
    "                O0[i % BATCH_SIZE] = jnp.pad(jnp.array(line['O0']),\n",
    "                                             pad_width=[(0, O0_MODEL_LINE_COUNT + 1 - len(line['O0'])), (0, 0)])\n",
    "                inp[i % BATCH_SIZE] = jnp.concatenate((jnp.pad(jnp.array(line['O0']),\n",
    "                                                               pad_width=[(0, MODEL_LINE_COUNT + 1 - len(line['O0'])),\n",
    "                                                                          (0, 0)]), jnp.pad(jnp.array(line['O2']),\n",
    "                                                                                            pad_width=[(0,\n",
    "                                                                                                        MODEL_LINE_COUNT + 1 - len(\n",
    "                                                                                                            line[\n",
    "                                                                                                                'O2'])),\n",
    "                                                                                                       (0, 0)]),\n",
    "                                                       jnp.pad(jnp.array(line['diff']),\n",
    "                                                               pad_width=[(0, MODEL_LINE_COUNT + 1 - len(line['diff'])),\n",
    "                                                                          (0, 0)])), axis=1)\n",
    "                outp[i % BATCH_SIZE] = pad_output(jax.nn.one_hot(jnp.array(\n",
    "                    vocab.to_seq(itertools.chain.from_iterable(map(lambda x: x + ['<sos>'], line['O2_tokens'])))[:-1]),\n",
    "                    VOCAB_SIZE), OUTPUT_TOKEN_COUNT, vocab.eos_index)\n",
    "                i += 1\n",
    "                if i % BATCH_SIZE == 0:\n",
    "                    yield O0, inp, outp"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.429169Z",
     "iopub.execute_input": "2022-02-11T09:05:08.429662Z",
     "iopub.status.idle": "2022-02-11T09:05:08.485867Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.429622Z",
     "shell.execute_reply": "2022-02-11T09:05:08.484654Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    teacher_force: bool\n",
    "\n",
    "    @functools.partial(\n",
    "        nn.transforms.scan,\n",
    "        variable_broadcast='params',\n",
    "        in_axes=1,\n",
    "        out_axes=1,\n",
    "        split_rngs={'params': False})\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        rng, lstm_state, last_prediction = carry\n",
    "        carry_rng, categorical_rng = jax.random.split(rng, 2)\n",
    "        if not self.teacher_force:\n",
    "            x = last_prediction\n",
    "        lstm_state, y = nn.LSTMCell()(lstm_state, x)\n",
    "        y = nn.Dense(features=1024)(y)\n",
    "        y = nn.Dense(features=1024)(y)\n",
    "        logits = nn.Dense(features=VOCAB_SIZE)(y)\n",
    "        predicted_token = jax.random.categorical(categorical_rng, logits)\n",
    "        prediction = jax.nn.one_hot(predicted_token, VOCAB_SIZE, dtype=jnp.float32)\n",
    "        return (carry_rng, lstm_state, prediction), (logits, prediction)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.489741Z",
     "iopub.execute_input": "2022-02-11T09:05:08.490560Z",
     "iopub.status.idle": "2022-02-11T09:05:08.505824Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.490508Z",
     "shell.execute_reply": "2022-02-11T09:05:08.504372Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    init_state: Tuple[Any]\n",
    "    teacher_force: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        # inputs.shape = (seq_length, vocab_size).\n",
    "        lstm = DecoderLSTM(teacher_force=self.teacher_force)\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        init_carry = (key, self.init_state, jnp.ones((BATCH_SIZE, VOCAB_SIZE), dtype=jnp.float32))\n",
    "        _, (logits, predictions) = lstm(init_carry, inputs)\n",
    "        return logits, predictions\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.507727Z",
     "iopub.execute_input": "2022-02-11T09:05:08.508039Z",
     "iopub.status.idle": "2022-02-11T09:05:08.528110Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.507998Z",
     "shell.execute_reply": "2022-02-11T09:05:08.526802Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_model(path, state):\n",
    "    with open(path, 'rb') as f:\n",
    "        saved_state = f.read()\n",
    "        return flax.serialization.from_bytes(state, saved_state)\n",
    "\n",
    "\n",
    "def save_model(state):\n",
    "    with open('model.bin', 'wb') as f:\n",
    "        f.write(flax.serialization.to_bytes(state.params))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.546277Z",
     "iopub.execute_input": "2022-02-11T09:05:08.547055Z",
     "iopub.status.idle": "2022-02-11T09:05:08.563247Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.546999Z",
     "shell.execute_reply": "2022-02-11T09:05:08.561825Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, enc_inputs, dec_inputs):\n",
    "        enc_result = Encoder(hidden_size=ENCODER_HIDDEN_SIZE)(enc_inputs)  # O0 + O2 + diff -> edit representation\n",
    "        decoder_init_state = enc_result\n",
    "        dec_result = Decoder(teacher_force=False, init_state=decoder_init_state)(dec_inputs)\n",
    "        return dec_result\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.565108Z",
     "iopub.execute_input": "2022-02-11T09:05:08.565974Z",
     "iopub.status.idle": "2022-02-11T09:05:08.579585Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.565925Z",
     "shell.execute_reply": "2022-02-11T09:05:08.578367Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def mask_sequences(sequence_batch, lengths):\n",
    "    \"\"\"Set positions beyond the length of each sequence to 0.\"\"\"\n",
    "    return sequence_batch * (\n",
    "            lengths[:, np.newaxis] > np.arange(sequence_batch.shape[1])[np.newaxis])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.581320Z",
     "iopub.execute_input": "2022-02-11T09:05:08.582016Z",
     "iopub.status.idle": "2022-02-11T09:05:08.594050Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.581958Z",
     "shell.execute_reply": "2022-02-11T09:05:08.592832Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(logits, labels, lengths):\n",
    "    \"\"\"Returns cross-entropy loss.\"\"\"\n",
    "    xe = jnp.sum(nn.log_softmax(logits) * labels, axis=-1)\n",
    "    masked_xe = jnp.sum(mask_sequences(xe, lengths)) / jnp.sum(lengths)\n",
    "    return -masked_xe\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.595938Z",
     "iopub.execute_input": "2022-02-11T09:05:08.596437Z",
     "iopub.status.idle": "2022-02-11T09:05:08.607844Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.596327Z",
     "shell.execute_reply": "2022-02-11T09:05:08.606462Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_sequence_lengths(sequence_batch, eos_id=vocab.eos_index):\n",
    "    \"\"\"Returns the length of each one-hot sequence, including the EOS token.\"\"\"\n",
    "    # sequence_batch.shape = (batch_size, seq_length, vocab_size)\n",
    "    eos_row = sequence_batch[:, :, eos_id]\n",
    "    eos_idx = jnp.argmax(eos_row, axis=-1)  # returns first occurrence\n",
    "    # `eos_idx` is 0 if EOS is not present, so we use full length in that case.\n",
    "    return jnp.where(\n",
    "        eos_row[jnp.arange(eos_row.shape[0]), eos_idx],\n",
    "        eos_idx + 1,\n",
    "        sequence_batch.shape[1]  # if there is no EOS, use full length\n",
    "    )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.609467Z",
     "iopub.execute_input": "2022-02-11T09:05:08.610126Z",
     "iopub.status.idle": "2022-02-11T09:05:08.621817Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.610074Z",
     "shell.execute_reply": "2022-02-11T09:05:08.620680Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"Computes metrics and returns them.\"\"\"\n",
    "    lengths = get_sequence_lengths(labels)\n",
    "    loss = cross_entropy_loss(logits, labels, lengths)\n",
    "    # Computes sequence accuracy, which is the same as the accuracy during\n",
    "    # inference, since teacher forcing is irrelevant when all output are correct.\n",
    "    token_accuracy = jnp.argmax(logits, -1) == jnp.argmax(labels, -1)\n",
    "    sequence_accuracy = (\n",
    "            jnp.sum(mask_sequences(token_accuracy, lengths), axis=-1) == lengths\n",
    "    )\n",
    "    accuracy = jnp.mean(sequence_accuracy)\n",
    "\n",
    "    predictions = jnp.argmax(logits[0], axis=-1)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'token_accuracy': jnp.sum(mask_sequences(token_accuracy, lengths)) / jnp.sum(lengths),\n",
    "        'predicted_out': labels[0, :, predictions],\n",
    "        'example_out': (labels[:1], logits[:1])\n",
    "\n",
    "    }\n",
    "    return metrics"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.623705Z",
     "iopub.execute_input": "2022-02-11T09:05:08.624030Z",
     "iopub.status.idle": "2022-02-11T09:05:08.644959Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.623990Z",
     "shell.execute_reply": "2022-02-11T09:05:08.643543Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    labels = batch['answer']\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits, _ = state.apply_fn({'params': params},\n",
    "                                   batch['query'],\n",
    "                                   batch['embed'])\n",
    "        loss = cross_entropy_loss(logits, labels, get_sequence_lengths(labels))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits, labels)\n",
    "\n",
    "    return state, metrics"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.646563Z",
     "iopub.execute_input": "2022-02-11T09:05:08.647084Z",
     "iopub.status.idle": "2022-02-11T09:05:08.659657Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.647050Z",
     "shell.execute_reply": "2022-02-11T09:05:08.658679Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(data, path=None):\n",
    "    \"\"\"Train for a fixed number of steps and decode during training.\"\"\"\n",
    "    global MODEL_LINE_COUNT, O0_MODEL_LINE_COUNT, OUTPUT_TOKEN_COUNT\n",
    "\n",
    "    MODEL_LINE_COUNT, O0_MODEL_LINE_COUNT, OUTPUT_TOKEN_COUNT = next(data)\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "\n",
    "    encoder_shape = jnp.ones((BATCH_SIZE, MODEL_LINE_COUNT + 1, SEQ_LENGTH), jnp.float32)\n",
    "    decoder_shape = jnp.ones((1, 1, 128), jnp.float32)\n",
    "    # key, init_key = jax.random.split(key)\n",
    "    model = Model()\n",
    "\n",
    "    loss = []\n",
    "    accuracy = []\n",
    "    perfect_accuracy = []\n",
    "\n",
    "    init_params = model.init({\"params\": key}, encoder_shape, decoder_shape)\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=init_params[\"params\"],\n",
    "                                          tx=optax.adam(LEARNING_RATE))\n",
    "    if path is not None:\n",
    "        state = state.replace(params=load_model(path, state.params))\n",
    "    for i, (O0, inp, outp) in zip(range(TRAIN_STEPS), data):\n",
    "        palmtree_avg = jnp.repeat(jnp.mean(O0, axis=1, keepdims=True), repeats=OUTPUT_TOKEN_COUNT, axis=1)\n",
    "        state, metrics = train_step(state, {'query': inp, 'answer': outp, 'embed': palmtree_avg})\n",
    "\n",
    "        loss.append(metrics[\"loss\"])\n",
    "        accuracy.append(metrics[\"token_accuracy\"])\n",
    "        perfect_accuracy.append(metrics[\"accuracy\"])\n",
    "\n",
    "        print(i, \"loss:\", metrics[\"loss\"], \"perfect_accuracy:\", metrics[\"accuracy\"], \"token_accuracy\",\n",
    "              metrics[\"token_accuracy\"])\n",
    "        print(\"EXAMPLE\", vocab.from_seq(jnp.argmax(metrics['example_out'][0][i], axis=-1)[\n",
    "                                        :get_sequence_lengths(metrics['example_out'][0][i][jnp.newaxis, :, :])[0]]))\n",
    "        max_indices = jnp.argmax(metrics['example_out'][1][i], axis=-1)\n",
    "        predicted = jax.nn.one_hot(max_indices, num_classes=VOCAB_SIZE)\n",
    "        res_lengths = get_sequence_lengths(predicted[jnp.newaxis, :, :])\n",
    "        print(\"RESULT\", vocab.from_seq(max_indices[:res_lengths[0]]))\n",
    "    return state, loss, accuracy, perfect_accuracy\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.661240Z",
     "iopub.execute_input": "2022-02-11T09:05:08.661740Z",
     "iopub.status.idle": "2022-02-11T09:05:08.688966Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.661696Z",
     "shell.execute_reply": "2022-02-11T09:05:08.687924Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model(data, state):\n",
    "    results = []\n",
    "    accuracy = 0\n",
    "    for i, (O0, inp, outp) in zip(range(NUM_OF_TESTS), data):\n",
    "        palmtree_avg = jnp.repeat(jnp.mean(O0, axis=1, keepdims=True), repeats=OUTPUT_TOKEN_COUNT, axis=1)\n",
    "        logits, _ = Model().apply({\"params\": state.params}, inp, palmtree_avg)\n",
    "        max_indices = jnp.argmax(logits, axis=-1)\n",
    "        predicted = jax.nn.one_hot(max_indices, num_classes=VOCAB_SIZE)\n",
    "        res_lengths = get_sequence_lengths(predicted)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            results.append(vocab.from_seq(max_indices[i, :res_lengths[i]]))\n",
    "\n",
    "        product = jnp.sum(predicted * outp, axis=-1)\n",
    "        accuracy += jnp.sum(mask_sequences(product, get_sequence_lengths(outp))) / jnp.sum(get_sequence_lengths(outp))\n",
    "\n",
    "    return {'accuracy': accuracy / NUM_OF_TESTS}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:05:08.691311Z",
     "iopub.execute_input": "2022-02-11T09:05:08.691849Z",
     "iopub.status.idle": "2022-02-11T09:05:08.709867Z",
     "shell.execute_reply.started": "2022-02-11T09:05:08.691806Z",
     "shell.execute_reply": "2022-02-11T09:05:08.708481Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = get_data()\n",
    "\n",
    "# To use pretrained model\n",
    "#state, loss, accuracy, perfect_accuracy = train_model('./model.bin')\n",
    "\n",
    "state, loss, accuracy, perfect_accuracy = train_model(data)\n",
    "print(test_model(data, state))\n",
    "save_model(state)\n",
    "\n",
    "\n",
    "# Plotting the important metrics\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(loss)\n",
    "ax1.set_ylabel(\"loss\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(accuracy)\n",
    "ax2.plot(perfect_accuracy)\n",
    "ax2.set_ylabel(\"accuracy\")\n",
    "fig.tight_layout()\n",
    "ax1.set_ylim(bottom=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T09:07:52.816483Z",
     "iopub.execute_input": "2022-02-11T09:07:52.817069Z",
     "iopub.status.idle": "2022-02-11T09:08:14.767511Z",
     "shell.execute_reply.started": "2022-02-11T09:07:52.817021Z",
     "shell.execute_reply": "2022-02-11T09:08:14.766341Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}